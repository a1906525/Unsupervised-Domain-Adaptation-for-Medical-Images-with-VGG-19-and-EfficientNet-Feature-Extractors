{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DevKh9sP4iWj"
      },
      "source": [
        "1) Load Google Colab, Mount the Directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GS8T11bx4ikq",
        "outputId": "32078897-2c70-4e27-dfea-b07edc539633"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yiz2lo9r5wmd"
      },
      "source": [
        "2) Inspect the Source - Number of Images Per Class, Sizes of Images Per Class, Channels of Images Per Class, and Total Number of Images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZnmeSI4q5wxe",
        "outputId": "2b5db664-7e75-43e7-f199-a3145a6ffc2e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Inspection of `/content/drive/MyDrive/Research Project 2025/Preprocessed Datasets/Samples/Source - MedMNIST Labelled/Images`\n",
            "Class         Images  Modes               Top Resolutions\n",
            "------------------------------------------------------------\n",
            "AbdomenCT     130     RGB:130             224×224:130\n",
            "BreastMRI     130     RGB:130             224×224:130\n",
            "CXR           130     RGB:130             224×224:130\n",
            "ChestCT       130     RGB:130             224×224:130\n",
            "HandXR        130     RGB:130             224×224:130\n",
            "HeadCT        130     RGB:130             224×224:130\n",
            "\n",
            "→ Total images: 780\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "from collections import Counter, defaultdict\n",
        "\n",
        "# ── Configuration ──────────────────────────────────────────────────────────\n",
        "src_dir = \"/content/drive/MyDrive/Research Project 2025/Preprocessed Datasets/Samples/Source - MedMNIST Labelled/Images\"\n",
        "# ── End configuration ──────────────────────────────────────────────────────\n",
        "\n",
        "def inspect_directory(path):\n",
        "    \"\"\"\n",
        "    Walk each class subfolder under `path`, count images,\n",
        "    record channel modes and resolutions, then print a summary.\n",
        "    \"\"\"\n",
        "    class_counts = Counter()\n",
        "    class_modes  = defaultdict(Counter)\n",
        "    class_res    = defaultdict(Counter)\n",
        "\n",
        "    for cls in sorted(os.listdir(path)):\n",
        "        cls_path = os.path.join(path, cls)\n",
        "        if not os.path.isdir(cls_path):\n",
        "            continue\n",
        "        for fname in os.listdir(cls_path):\n",
        "            if not fname.lower().endswith(('.png','.jpg','.jpeg','.bmp','.tiff')):\n",
        "                continue\n",
        "            class_counts[cls] += 1\n",
        "            img_path = os.path.join(cls_path, fname)\n",
        "            try:\n",
        "                with Image.open(img_path) as img:\n",
        "                    class_modes[cls][img.mode] += 1\n",
        "                    class_res[cls][img.size] += 1\n",
        "            except Exception as e:\n",
        "                print(f\"❌ Error opening {img_path}: {e}\")\n",
        "\n",
        "    total = sum(class_counts.values())\n",
        "    print(f\"\\nInspection of `{path}`\")\n",
        "    print(f\"{'Class':<12}  Images  Modes               Top Resolutions\")\n",
        "    print(\"-\"*60)\n",
        "    for cls, cnt in class_counts.items():\n",
        "        modes_s = \", \".join(f\"{m}:{n}\" for m,n in class_modes[cls].items())\n",
        "        top3    = class_res[cls].most_common(3)\n",
        "        res_s   = \", \".join(f\"{w}×{h}:{n}\" for (w,h),n in top3)\n",
        "        print(f\"{cls:<12}  {cnt:<6}  {modes_s:<18}  {res_s}\")\n",
        "    print(f\"\\n→ Total images: {total}\\n\")\n",
        "\n",
        "# Run the inspection\n",
        "inspect_directory(src_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOi1ERjr7RK0"
      },
      "source": [
        "3) Inspect the Target (unlabelled) dataset if it has 780 images, 224x224 size, and 3 channels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cw-Ephgc7RWd",
        "outputId": "13e63904-552f-4626-f707-3f953a0eb2aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Inspection of unlabelled target directory: `/content/drive/MyDrive/Research Project 2025/Preprocessed Datasets/Samples/Target - VS Unlabelled/Images`\n",
            "→ Total images: 780\n",
            "\n",
            "Channel modes:\n",
            "  RGB: 780\n",
            "\n",
            "Image resolutions:\n",
            "  224×224: 780\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "from collections import Counter\n",
        "\n",
        "# ── Configuration ──────────────────────────────────────────────────────────\n",
        "unlabelled_dir = \"/content/drive/MyDrive/Research Project 2025/Preprocessed Datasets/Samples/Target - VS Unlabelled/Images\"\n",
        "# ── End configuration ──────────────────────────────────────────────────────\n",
        "\n",
        "# Counters for total, modes, and resolutions\n",
        "total_images = 0\n",
        "mode_counts  = Counter()\n",
        "size_counts  = Counter()\n",
        "\n",
        "# Iterate through all images\n",
        "for fname in os.listdir(unlabelled_dir):\n",
        "    if not fname.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff')):\n",
        "        continue\n",
        "    total_images += 1\n",
        "    img_path = os.path.join(unlabelled_dir, fname)\n",
        "    try:\n",
        "        with Image.open(img_path) as img:\n",
        "            mode_counts[img.mode] += 1\n",
        "            size_counts[img.size] += 1\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error opening {img_path}: {e}\")\n",
        "\n",
        "# Display results\n",
        "print(f\"\\nInspection of unlabelled target directory: `{unlabelled_dir}`\")\n",
        "print(f\"→ Total images: {total_images}\\n\")\n",
        "\n",
        "print(\"Channel modes:\")\n",
        "for mode, cnt in mode_counts.items():\n",
        "    print(f\"  {mode}: {cnt}\")\n",
        "\n",
        "print(\"\\nImage resolutions:\")\n",
        "for (w, h), cnt in size_counts.most_common():\n",
        "    print(f\"  {w}×{h}: {cnt}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZ7Vi8ev8H8T"
      },
      "source": [
        "4) Let us Normalize the source."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cYSYNCZT8IJQ",
        "outputId": "d46f8ca1-c35e-433c-c750-270e0b9a37db"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Pre-norm stats: 100%|██████████| 13/13 [00:05<00:00,  2.32it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pre-normalization mean: tensor([0.3542, 0.3542, 0.3542])\n",
            "Pre-normalization std:  tensor([0.2794, 0.2794, 0.2794])\n",
            "\n",
            "One-batch post-norm mean: tensor([0.3144, 0.3144, 0.3144])\n",
            "One-batch post-norm std:  tensor([0.3413, 0.3413, 0.3413])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Post-norm full stats: 100%|██████████| 13/13 [00:04<00:00,  2.78it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Post-normalization mean: tensor([3.1939e-08, 3.1939e-08, 3.1939e-08])\n",
            "Post-normalization std:  tensor([1.0000, 1.0000, 1.0000])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ── Configuration ────────────────────────────────────────────────────────────\n",
        "data_dir = \"/content/drive/MyDrive/Research Project 2025/Preprocessed Datasets/Samples/Source - MedMNIST Labelled/Images\"\n",
        "batch_size = 64   # for batch-level stats\n",
        "num_workers = 4\n",
        "# ── End configuration ─────────────────────────────────────────────────────────\n",
        "\n",
        "# 1) Pre-normalization: load as Tensor [0,1] but no Normalize\n",
        "pre_transform = transforms.Compose([\n",
        "    transforms.ToTensor()   # converts to [C,H,W] in [0.0,1.0]\n",
        "])\n",
        "pre_ds = datasets.ImageFolder(data_dir, transform=pre_transform)\n",
        "pre_loader = DataLoader(pre_ds, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
        "\n",
        "# Compute channel sums & squared sums\n",
        "n_channels = 3\n",
        "cnt = 0\n",
        "sum_ = torch.zeros(n_channels)\n",
        "sum_sq = torch.zeros(n_channels)\n",
        "\n",
        "for imgs, _ in tqdm(pre_loader, desc=\"Pre-norm stats\"):\n",
        "    # imgs shape: [B, C, H, W]\n",
        "    b, c, h, w = imgs.shape\n",
        "    cnt += b * h * w\n",
        "    sum_ += imgs.sum(dim=[0,2,3])\n",
        "    sum_sq += (imgs ** 2).sum(dim=[0,2,3])\n",
        "\n",
        "mean_pre = sum_ / cnt\n",
        "var_pre = (sum_sq / cnt) - (mean_pre ** 2)\n",
        "std_pre = torch.sqrt(var_pre)\n",
        "\n",
        "print(\"Pre-normalization mean:\", mean_pre)\n",
        "print(\"Pre-normalization std: \", std_pre)\n",
        "\n",
        "\n",
        "# 2) One-batch post-normalization: take first batch with Normalize()\n",
        "normalize = transforms.Normalize(mean=mean_pre.tolist(), std=std_pre.tolist())\n",
        "batch_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    normalize\n",
        "])\n",
        "batch_ds = datasets.ImageFolder(data_dir, transform=batch_transform)\n",
        "batch_loader = DataLoader(batch_ds, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
        "\n",
        "# Get one batch\n",
        "imgs_batch, _ = next(iter(batch_loader))  # [B, C, H, W]\n",
        "mean_batch = imgs_batch.mean(dim=[0,2,3])\n",
        "std_batch  = imgs_batch.std(dim=[0,2,3])\n",
        "\n",
        "print(\"\\nOne-batch post-norm mean:\", mean_batch)\n",
        "print(\"One-batch post-norm std: \", std_batch)\n",
        "\n",
        "\n",
        "# 3) Full-dataset post-normalization: entire loader with Normalize\n",
        "post_loader = DataLoader(batch_ds, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
        "\n",
        "cnt2 = 0\n",
        "sum2 = torch.zeros(n_channels)\n",
        "sum2_sq = torch.zeros(n_channels)\n",
        "\n",
        "for imgs, _ in tqdm(post_loader, desc=\"Post-norm full stats\"):\n",
        "    b, c, h, w = imgs.shape\n",
        "    cnt2 += b * h * w\n",
        "    sum2 += imgs.sum(dim=[0,2,3])\n",
        "    sum2_sq += (imgs ** 2).sum(dim=[0,2,3])\n",
        "\n",
        "mean_post = sum2 / cnt2\n",
        "var_post = (sum2_sq / cnt2) - (mean_post ** 2)\n",
        "std_post = torch.sqrt(var_post)\n",
        "\n",
        "print(\"\\nPost-normalization mean:\", mean_post)\n",
        "print(\"Post-normalization std: \", std_post)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2KC76YDy-EST"
      },
      "source": [
        "5) Let us build the VGG-19 architecture.\n",
        "Inspired from : https://arxiv.org/abs/1409.1556"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2dIlmoNr-Edh",
        "outputId": "35260f9d-25dd-4180-f430-66e6fef1ab7d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
            "Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.8.0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "VGG19                                    [1, 6]                    --\n",
              "├─Sequential: 1-1                        [1, 512, 7, 7]            --\n",
              "│    └─Conv2d: 2-1                       [1, 64, 224, 224]         1,792\n",
              "│    └─ReLU: 2-2                         [1, 64, 224, 224]         --\n",
              "│    └─Conv2d: 2-3                       [1, 64, 224, 224]         36,928\n",
              "│    └─ReLU: 2-4                         [1, 64, 224, 224]         --\n",
              "│    └─MaxPool2d: 2-5                    [1, 64, 112, 112]         --\n",
              "│    └─Conv2d: 2-6                       [1, 128, 112, 112]        73,856\n",
              "│    └─ReLU: 2-7                         [1, 128, 112, 112]        --\n",
              "│    └─Conv2d: 2-8                       [1, 128, 112, 112]        147,584\n",
              "│    └─ReLU: 2-9                         [1, 128, 112, 112]        --\n",
              "│    └─MaxPool2d: 2-10                   [1, 128, 56, 56]          --\n",
              "│    └─Conv2d: 2-11                      [1, 256, 56, 56]          295,168\n",
              "│    └─ReLU: 2-12                        [1, 256, 56, 56]          --\n",
              "│    └─Conv2d: 2-13                      [1, 256, 56, 56]          590,080\n",
              "│    └─ReLU: 2-14                        [1, 256, 56, 56]          --\n",
              "│    └─Conv2d: 2-15                      [1, 256, 56, 56]          590,080\n",
              "│    └─ReLU: 2-16                        [1, 256, 56, 56]          --\n",
              "│    └─Conv2d: 2-17                      [1, 256, 56, 56]          590,080\n",
              "│    └─ReLU: 2-18                        [1, 256, 56, 56]          --\n",
              "│    └─MaxPool2d: 2-19                   [1, 256, 28, 28]          --\n",
              "│    └─Conv2d: 2-20                      [1, 512, 28, 28]          1,180,160\n",
              "│    └─ReLU: 2-21                        [1, 512, 28, 28]          --\n",
              "│    └─Conv2d: 2-22                      [1, 512, 28, 28]          2,359,808\n",
              "│    └─ReLU: 2-23                        [1, 512, 28, 28]          --\n",
              "│    └─Conv2d: 2-24                      [1, 512, 28, 28]          2,359,808\n",
              "│    └─ReLU: 2-25                        [1, 512, 28, 28]          --\n",
              "│    └─Conv2d: 2-26                      [1, 512, 28, 28]          2,359,808\n",
              "│    └─ReLU: 2-27                        [1, 512, 28, 28]          --\n",
              "│    └─MaxPool2d: 2-28                   [1, 512, 14, 14]          --\n",
              "│    └─Conv2d: 2-29                      [1, 512, 14, 14]          2,359,808\n",
              "│    └─ReLU: 2-30                        [1, 512, 14, 14]          --\n",
              "│    └─Conv2d: 2-31                      [1, 512, 14, 14]          2,359,808\n",
              "│    └─ReLU: 2-32                        [1, 512, 14, 14]          --\n",
              "│    └─Conv2d: 2-33                      [1, 512, 14, 14]          2,359,808\n",
              "│    └─ReLU: 2-34                        [1, 512, 14, 14]          --\n",
              "│    └─Conv2d: 2-35                      [1, 512, 14, 14]          2,359,808\n",
              "│    └─ReLU: 2-36                        [1, 512, 14, 14]          --\n",
              "│    └─MaxPool2d: 2-37                   [1, 512, 7, 7]            --\n",
              "├─AdaptiveAvgPool2d: 1-2                 [1, 512, 7, 7]            --\n",
              "├─Sequential: 1-3                        [1, 6]                    --\n",
              "│    └─Linear: 2-38                      [1, 4096]                 102,764,544\n",
              "│    └─ReLU: 2-39                        [1, 4096]                 --\n",
              "│    └─Dropout: 2-40                     [1, 4096]                 --\n",
              "│    └─Linear: 2-41                      [1, 4096]                 16,781,312\n",
              "│    └─ReLU: 2-42                        [1, 4096]                 --\n",
              "│    └─Dropout: 2-43                     [1, 4096]                 --\n",
              "│    └─Linear: 2-44                      [1, 6]                    24,582\n",
              "==========================================================================================\n",
              "Total params: 139,594,822\n",
              "Trainable params: 139,594,822\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (Units.GIGABYTES): 19.64\n",
              "==========================================================================================\n",
              "Input size (MB): 0.60\n",
              "Forward/backward pass size (MB): 118.88\n",
              "Params size (MB): 558.38\n",
              "Estimated Total Size (MB): 677.86\n",
              "=========================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Classic VGG-19 \"E\" configuration: 16 conv layers + 5 max pooling\n",
        "cfg_vgg19 = [\n",
        "    64, 64, \"M\",\n",
        "    128, 128, \"M\",\n",
        "    256, 256, 256, 256, \"M\",\n",
        "    512, 512, 512, 512, \"M\",\n",
        "    512, 512, 512, 512, \"M\"\n",
        "]\n",
        "\n",
        "def make_layers(cfg):\n",
        "    layers = []\n",
        "    in_channels = 3\n",
        "    for v in cfg:\n",
        "        if v == \"M\":\n",
        "            layers.append(nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "        else:\n",
        "            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n",
        "            layers += [conv2d, nn.ReLU(inplace=True)]\n",
        "            in_channels = v\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "class VGG19(nn.Module):\n",
        "    def __init__(self, num_classes=6, init_weights=True):\n",
        "        super().__init__()\n",
        "        self.features = make_layers(cfg_vgg19)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((7, 7))\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(512*7*7, 4096),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(4096, num_classes)\n",
        "        )\n",
        "        if init_weights:\n",
        "            self._initialize_weights()\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.normal_(m.weight, 0, 0.01)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "# Instantiate the model for 6 classes\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = VGG19(num_classes=6).to(device)\n",
        "\n",
        "# Visualize model summary (install torchinfo if needed)\n",
        "try:\n",
        "    from torchinfo import summary\n",
        "except ImportError:\n",
        "    import sys\n",
        "    # Install torchinfo using pip for the current python executable\n",
        "    !{sys.executable} -m pip install torchinfo\n",
        "    from torchinfo import summary\n",
        "\n",
        "summary(model, input_size=(1, 3, 224, 224))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "psdCZmQeATS_"
      },
      "source": [
        "6) Let us prepare the data loaders for split."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x5wwvCsNATgX",
        "outputId": "d457b2c7-5f54-4454-90e6-57a09cb4a0a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Classes: ['AbdomenCT', 'BreastMRI', 'CXR', 'ChestCT', 'HandXR', 'HeadCT']\n",
            "Train size: 585, Val size: 78, Test size: 117\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import torch\n",
        "import os\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from torch.utils.data.dataset import random_split\n",
        "\n",
        "# Paths\n",
        "source_dir = '/content/drive/MyDrive/Research Project 2025/Preprocessed Datasets/Samples/Source - MedMNIST Labelled/Images'\n",
        "\n",
        "# Precomputed normalization stats\n",
        "mean = [0.3542, 0.3542, 0.3542]\n",
        "std  = [0.2794, 0.2794, 0.2794]\n",
        "\n",
        "# Transforms\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(8),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std)\n",
        "])\n",
        "val_test_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std)\n",
        "])\n",
        "\n",
        "# 1. Load base dataset (no transform yet)\n",
        "base_ds = ImageFolder(source_dir, transform=None)\n",
        "\n",
        "# 2. Compute split sizes and indices\n",
        "n_total = len(base_ds)\n",
        "n_train = int(0.75 * n_total)\n",
        "n_val   = int(0.10 * n_total)\n",
        "n_test  = n_total - n_train - n_val\n",
        "\n",
        "# Use random_split for reproducible indices\n",
        "train_ds_idx, val_ds_idx, test_ds_idx = random_split(\n",
        "    list(range(n_total)),\n",
        "    [n_train, n_val, n_test],\n",
        "    generator=torch.Generator().manual_seed(1906525)\n",
        ")\n",
        "\n",
        "# 3. Create Subsets with appropriate transforms\n",
        "train_ds = Subset(\n",
        "    ImageFolder(source_dir, transform=train_transform), train_ds_idx\n",
        ")\n",
        "val_ds   = Subset(\n",
        "    ImageFolder(source_dir, transform=val_test_transform), val_ds_idx\n",
        ")\n",
        "test_ds  = Subset(\n",
        "    ImageFolder(source_dir, transform=val_test_transform), test_ds_idx\n",
        ")\n",
        "\n",
        "# 4. DataLoaders\n",
        "batch_size  = 32\n",
        "num_workers = 4\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True,  num_workers=num_workers)\n",
        "val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
        "test_loader  = DataLoader(test_ds,  batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
        "\n",
        "# 5. Sanity check\n",
        "print(\"Classes:\", base_ds.classes)\n",
        "print(f\"Train size: {len(train_ds)}, Val size: {len(val_ds)}, Test size: {len(test_ds)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZ66gyDiF9Z3"
      },
      "source": [
        "7) Let us start modelling for the source MedMNIST dataset samples with VGG-19 and save the checkpoints."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ITrq3TOiF9k_",
        "outputId": "b0e298c7-37b4-48b0-fc14-372f2ae31312"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "\n",
            "▶ Run: SGD_ep5\n",
            "Completed SGD for 5 epochs → Train Acc: 0.4957, Test Acc: 0.4615\n",
            "\n",
            "▶ Run: SGD_ep10\n",
            "Completed SGD for 10 epochs → Train Acc: 0.8291, Test Acc: 0.8034\n",
            "\n",
            "▶ Run: SGD_ep20\n",
            "Completed SGD for 20 epochs → Train Acc: 0.9556, Test Acc: 0.9402\n",
            "\n",
            "▶ Run: SGD_ep40\n",
            "Completed SGD for 40 epochs → Train Acc: 0.8427, Test Acc: 0.8462\n",
            "\n",
            "▶ Run: SGD_ep80\n",
            "Completed SGD for 80 epochs → Train Acc: 0.9795, Test Acc: 0.9658\n",
            "\n",
            "▶ Run: Adam_ep5\n",
            "Completed Adam for 5 epochs → Train Acc: 0.8427, Test Acc: 0.8034\n",
            "\n",
            "▶ Run: Adam_ep10\n",
            "Completed Adam for 10 epochs → Train Acc: 0.1658, Test Acc: 0.1538\n",
            "\n",
            "▶ Run: Adam_ep20\n",
            "Completed Adam for 20 epochs → Train Acc: 0.9880, Test Acc: 0.9658\n",
            "\n",
            "▶ Run: Adam_ep40\n",
            "Completed Adam for 40 epochs → Train Acc: 0.1692, Test Acc: 0.1709\n",
            "\n",
            "▶ Run: Adam_ep80\n",
            "Completed Adam for 80 epochs → Train Acc: 0.8274, Test Acc: 0.8120\n",
            "\n",
            "▶ Run: RMSprop_ep5\n",
            "Completed RMSprop for 5 epochs → Train Acc: 0.8274, Test Acc: 0.8291\n",
            "\n",
            "▶ Run: RMSprop_ep10\n",
            "Completed RMSprop for 10 epochs → Train Acc: 0.7299, Test Acc: 0.7094\n",
            "\n",
            "▶ Run: RMSprop_ep20\n",
            "Completed RMSprop for 20 epochs → Train Acc: 0.9538, Test Acc: 0.9060\n",
            "\n",
            "▶ Run: RMSprop_ep40\n",
            "Completed RMSprop for 40 epochs → Train Acc: 0.9863, Test Acc: 0.9915\n",
            "\n",
            "▶ Run: RMSprop_ep80\n",
            "Completed RMSprop for 80 epochs → Train Acc: 0.9880, Test Acc: 0.9487\n",
            "\n",
            "▶ Run: Adagrad_ep5\n",
            "Completed Adagrad for 5 epochs → Train Acc: 0.9726, Test Acc: 0.9658\n",
            "\n",
            "▶ Run: Adagrad_ep10\n",
            "Completed Adagrad for 10 epochs → Train Acc: 0.9265, Test Acc: 0.9145\n",
            "\n",
            "▶ Run: Adagrad_ep20\n",
            "Completed Adagrad for 20 epochs → Train Acc: 1.0000, Test Acc: 0.9915\n",
            "\n",
            "▶ Run: Adagrad_ep40\n",
            "Completed Adagrad for 40 epochs → Train Acc: 1.0000, Test Acc: 1.0000\n",
            "\n",
            "▶ Run: Adagrad_ep80\n",
            "Completed Adagrad for 80 epochs → Train Acc: 1.0000, Test Acc: 0.9829\n",
            "\n",
            "▶ Run: AdamW_ep5\n",
            "Completed AdamW for 5 epochs → Train Acc: 0.9111, Test Acc: 0.9231\n",
            "\n",
            "▶ Run: AdamW_ep10\n",
            "Completed AdamW for 10 epochs → Train Acc: 0.1675, Test Acc: 0.1795\n",
            "\n",
            "▶ Run: AdamW_ep20\n",
            "Completed AdamW for 20 epochs → Train Acc: 0.8718, Test Acc: 0.8205\n",
            "\n",
            "▶ Run: AdamW_ep40\n",
            "Completed AdamW for 40 epochs → Train Acc: 0.9897, Test Acc: 0.8120\n",
            "\n",
            "▶ Run: AdamW_ep80\n",
            "Completed AdamW for 80 epochs → Train Acc: 0.1692, Test Acc: 0.1709\n",
            "Training completed, saved the results for MedMNIST (samples) with VGG‑19 model.\n"
          ]
        }
      ],
      "source": [
        "# ─── 0. Mount & Imports ─────────────────────────────────────────────────────\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os, random, numpy as np\n",
        "import torch, torch.nn as nn, torch.backends.cudnn as cudnn\n",
        "import pandas as pd, seaborn as sns, matplotlib.pyplot as plt\n",
        "\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from torch.utils.data.dataset import random_split\n",
        "from torch.optim import SGD, Adam, RMSprop, Adagrad, AdamW\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score,\n",
        "    f1_score, roc_auc_score, mean_squared_error,\n",
        "    confusion_matrix\n",
        ")\n",
        "\n",
        "# ─── 1. Determinism ─────────────────────────────────────────────────────────\n",
        "SEED = 1906525\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "cudnn.deterministic = True\n",
        "cudnn.benchmark = False\n",
        "\n",
        "# ─── 2. Paths & Hyperparams ─────────────────────────────────────────────────\n",
        "SRC_DIR = '/content/drive/MyDrive/Research Project 2025/Preprocessed Datasets/Samples/Source - MedMNIST Labelled/Images'\n",
        "\n",
        "CKPT_DIR    = '/content/drive/MyDrive/Research Project 2025/Results/Samples/VGG-19/Source- Training/Checkpoints'\n",
        "CM_DIR      = '/content/drive/MyDrive/Research Project 2025/Results/Samples/VGG-19/Source- Training/Confusion Matrices'\n",
        "METRICS_DIR = '/content/drive/MyDrive/Research Project 2025/Results/Samples/VGG-19/Source- Training/Performance Metrics'\n",
        "os.makedirs(CKPT_DIR, exist_ok=True)\n",
        "os.makedirs(CM_DIR, exist_ok=True)\n",
        "os.makedirs(METRICS_DIR, exist_ok=True)\n",
        "\n",
        "MEAN = [0.3542, 0.3542, 0.3542]\n",
        "STD  = [0.2794, 0.2794, 0.2794]\n",
        "\n",
        "train_tf = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(8),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(MEAN, STD)\n",
        "])\n",
        "val_tf = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(MEAN, STD)\n",
        "])\n",
        "\n",
        "OPTIMIZERS   = [SGD, Adam, RMSprop, Adagrad, AdamW]\n",
        "BATCH_SIZE   = 32\n",
        "EPOCHS_LIST  = [5, 10, 20, 40, 80]\n",
        "LR           = 1e-3\n",
        "DEVICE       = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# ─── 3. Prepare DataLoaders ─────────────────────────────────────────────────\n",
        "base_ds = ImageFolder(SRC_DIR, transform=None)\n",
        "n = len(base_ds)\n",
        "n_train = int(0.75 * n)\n",
        "n_val   = int(0.10 * n)\n",
        "n_test  = n - n_train - n_val\n",
        "\n",
        "idx_train, idx_val, idx_test = random_split(\n",
        "    list(range(n)), [n_train, n_val, n_test],\n",
        "    generator=torch.Generator().manual_seed(SEED)\n",
        ")\n",
        "\n",
        "train_ds = Subset(ImageFolder(SRC_DIR, transform=train_tf), idx_train)\n",
        "val_ds   = Subset(ImageFolder(SRC_DIR, transform=val_tf),   idx_val)\n",
        "test_ds  = Subset(ImageFolder(SRC_DIR, transform=val_tf),   idx_test)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,  num_workers=4)\n",
        "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
        "test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
        "\n",
        "# ─── 4. VGG-19 Definition ────────────────────────────────────────────────────\n",
        "cfg = [64,64,'M',128,128,'M',256,256,256,256,'M',512,512,512,512,'M',512,512,512,512,'M']\n",
        "def make_layers(cfg):\n",
        "    layers, in_c = [], 3\n",
        "    for v in cfg:\n",
        "        if v == 'M':\n",
        "            layers.append(nn.MaxPool2d(2,2))\n",
        "        else:\n",
        "            layers += [nn.Conv2d(in_c, v, 3, padding=1), nn.ReLU(True)]\n",
        "            in_c = v\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "class VGG19(nn.Module):\n",
        "    def __init__(self, num_classes=6):\n",
        "        super().__init__()\n",
        "        self.features = make_layers(cfg)\n",
        "        self.avgpool  = nn.AdaptiveAvgPool2d((7,7))\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(512*7*7, 4096), nn.ReLU(True), nn.Dropout(0.5),\n",
        "            nn.Linear(4096, 4096),    nn.ReLU(True), nn.Dropout(0.5),\n",
        "            nn.Linear(4096, num_classes)\n",
        "        )\n",
        "        self._init_weights()\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        return self.classifier(x)\n",
        "    def _init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None: nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.normal_(m.weight, 0, 0.01); nn.init.constant_(m.bias, 0)\n",
        "\n",
        "def evaluate_split(model, loader, criterion=None, plot_cm=False, cm_name=None):\n",
        "    model.eval()\n",
        "    all_preds, all_labels, all_probs = [], [], []\n",
        "    total_loss, total_samples = 0.0, 0\n",
        "    with torch.no_grad():\n",
        "        for x,y in loader:\n",
        "            x, y = x.to(DEVICE), y.to(DEVICE)\n",
        "            logits = model(x)\n",
        "            if criterion:\n",
        "                total_loss += criterion(logits, y).item() * x.size(0)\n",
        "            probs = torch.softmax(logits,1).cpu().numpy()\n",
        "            preds = logits.argmax(1).cpu().numpy()\n",
        "            all_probs.extend(probs)\n",
        "            all_preds.extend(preds)\n",
        "            all_labels.extend(y.cpu().numpy())\n",
        "            total_samples += x.size(0)\n",
        "\n",
        "    acc = accuracy_score(all_labels, all_preds)\n",
        "    prec = precision_score(all_labels, all_preds, average='macro', zero_division=0)\n",
        "    rec = recall_score(all_labels, all_preds, average='macro', zero_division=0)\n",
        "    f1 = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n",
        "    try:\n",
        "        auc = roc_auc_score(all_labels, all_probs, multi_class='ovo')\n",
        "    except:\n",
        "        auc = float('nan')\n",
        "    mse = mean_squared_error(all_labels, all_preds)\n",
        "\n",
        "    cm = confusion_matrix(all_labels, all_preds, labels=list(range(len(loader.dataset.dataset.classes))))\n",
        "    total = cm.sum()\n",
        "    specs = []\n",
        "    for i in range(cm.shape[0]):\n",
        "        tp = cm[i,i]\n",
        "        fn = cm[i,:].sum() - tp\n",
        "        fp = cm[:,i].sum() - tp\n",
        "        tn = total - tp - fp - fn\n",
        "        specs.append(tn/(tn+fp) if (tn+fp) > 0 else 0.)\n",
        "    spec = float(np.mean(specs))\n",
        "\n",
        "    if plot_cm and cm_name:\n",
        "        plt.figure(figsize=(8,6))\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                    xticklabels=loader.dataset.dataset.classes,\n",
        "                    yticklabels=loader.dataset.dataset.classes)\n",
        "        plt.xlabel('Predicted'); plt.ylabel('Actual')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(CM_DIR, cm_name))\n",
        "        plt.close()\n",
        "\n",
        "    loss = total_loss / total_samples if criterion else None\n",
        "    return acc, prec, rec, spec, f1, auc, mse, loss\n",
        "\n",
        "# ─── 5. Training & Logging ─────────────────────────────────────────────────\n",
        "master = []\n",
        "\n",
        "for opt_cls in OPTIMIZERS:\n",
        "    for num_epochs in EPOCHS_LIST:\n",
        "        print(f\"\\n▶ Run: {opt_cls.__name__}_ep{num_epochs}\")\n",
        "        model = VGG19(num_classes=len(base_ds.classes)).to(DEVICE)\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimizer = opt_cls(model.parameters(), lr=LR)\n",
        "        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.1, patience=5)\n",
        "\n",
        "        def get_grad_norm():\n",
        "            model.eval(); model.zero_grad()\n",
        "            x,y = next(iter(train_loader))\n",
        "            loss = criterion(model(x.to(DEVICE)), y.to(DEVICE))\n",
        "            loss.backward()\n",
        "            return sum(p.grad.norm().item()**2 for p in model.parameters() if p.grad is not None)**0.5\n",
        "\n",
        "        grad_before = get_grad_norm()\n",
        "\n",
        "        for epoch in range(1, num_epochs+1):\n",
        "            model.train()\n",
        "            for x,y in train_loader:\n",
        "                x,y = x.to(DEVICE), y.to(DEVICE)\n",
        "                optimizer.zero_grad()\n",
        "                loss = criterion(model(x), y)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "            _,_,_,_,_,_,_, val_loss = evaluate_split(model, val_loader, criterion)\n",
        "            scheduler.step(val_loss)\n",
        "\n",
        "            if epoch == 40:\n",
        "                evaluate_split(model, test_loader, plot_cm=True, cm_name=f\"{opt_cls.__name__}_ep40.png\")\n",
        "\n",
        "        train_acc, train_prec, train_rec, spec, f1, auc, train_mse, _ = evaluate_split(model, train_loader)\n",
        "        val_acc, val_prec, val_rec, val_spec, val_f1, val_auc, val_mse, _ = evaluate_split(model, val_loader)\n",
        "        test_acc, test_prec, test_rec, test_spec, test_f1, test_auc, test_mse, _ = evaluate_split(model, test_loader)\n",
        "        grad_after = get_grad_norm()\n",
        "\n",
        "        print(f\"Completed {opt_cls.__name__} for {num_epochs} epochs → Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}\")\n",
        "\n",
        "        torch.save(model.state_dict(), os.path.join(CKPT_DIR, f\"{opt_cls.__name__}_ep{num_epochs}.pth\"))\n",
        "\n",
        "        master.append({\n",
        "            'Optimizer': opt_cls.__name__, 'Epochs': num_epochs,\n",
        "            'train_acc': train_acc, 'val_acc': val_acc, 'test_acc': test_acc,\n",
        "            'train_mse': train_mse, 'val_mse': val_mse, 'test_mse': test_mse,\n",
        "            'precision': test_prec, 'sensitivity': test_rec,\n",
        "            'specificity': test_spec, 'f1': test_f1, 'auc': test_auc,\n",
        "            'grad_before': grad_before, 'grad_after': grad_after\n",
        "        })\n",
        "\n",
        "metrics_file = os.path.join(METRICS_DIR, 'VGG_19_sample_source.xlsx')\n",
        "pd.DataFrame(master).to_excel(metrics_file, index=False)\n",
        "print(\"Training completed, saved the results for MedMNIST (samples) with VGG‑19 model.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OB1iO6-v3skT"
      },
      "source": [
        "8) Let us inspect the target (unlabelled) dataset's channels and image sizes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nJWm4c_w3sxs",
        "outputId": "3df18bb1-0e7e-4783-e658-23c9d4745fd2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total image files: 780\n",
            "\n",
            "Image modes (counts):\n",
            "  Mode: RGB, Count: 780\n",
            "\n",
            "Unique image dimensions (width × height) and their counts:\n",
            "  224×224: 780 images\n",
            "\n",
            "Channels per mode:\n",
            "  Mode: RGB, Channels: 3\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "from collections import Counter\n",
        "from PIL import Image\n",
        "\n",
        "TARGET_DIR = Path(\"/content/drive/MyDrive/Research Project 2025/Preprocessed Datasets/Samples/Target - VS Unlabelled/Images\")\n",
        "\n",
        "IMAGE_EXTS = {'.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.tif'}\n",
        "files = [f for f in TARGET_DIR.rglob(\"*\") if f.suffix.lower() in IMAGE_EXTS and f.is_file()]\n",
        "\n",
        "print(f\"Total image files: {len(files)}\")\n",
        "\n",
        "mode_counter = Counter()\n",
        "size_counter = Counter()\n",
        "dimensions = []\n",
        "\n",
        "for f in files:\n",
        "    try:\n",
        "        with Image.open(f) as img:\n",
        "            mode = img.mode  # e.g., 'RGB', 'L', etc.\n",
        "            width, height = img.size\n",
        "            mode_counter[mode] += 1\n",
        "            size_counter[(width, height)] += 1\n",
        "            dimensions.append((width, height))\n",
        "    except Exception as e:\n",
        "        print(f\"Error with {f}: {e}\")\n",
        "\n",
        "print(\"\\nImage modes (counts):\")\n",
        "for mode, count in mode_counter.items():\n",
        "    print(f\"  Mode: {mode}, Count: {count}\")\n",
        "\n",
        "print(\"\\nUnique image dimensions (width × height) and their counts:\")\n",
        "for (w, h), count in size_counter.most_common():\n",
        "    print(f\"  {w}×{h}: {count} images\")\n",
        "\n",
        "# Function to get channels from mode\n",
        "def channels_from_mode(mode):\n",
        "    # As noted on StackOverflow, image.getbands() gives accurate channel count\n",
        "    return len(Image.new(mode, (1,1)).getbands())\n",
        "\n",
        "print(\"\\nChannels per mode:\")\n",
        "for mode in mode_counter:\n",
        "    print(f\"  Mode: {mode}, Channels: {channels_from_mode(mode)}\")\n",
        "\n",
        "# Optional: Compute min/max/avg dimensions\n",
        "# if dimensions:\n",
        "#     ws = [w for w, _ in dimensions]\n",
        "#     hs = [h for _, h in dimensions]\n",
        "#     print(f\"\\nWidth — min: {min(ws)}, max: {max(ws)}, avg: {sum(ws)/len(ws):.2f}\")\n",
        "#     print(f\"Height — min: {min(hs)}, max: {max(hs)}, avg: {sum(hs)/len(hs):.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Ayrm_yCEPlx"
      },
      "source": [
        "9) Let us do normalization for the target (unlabelled) dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_NyM38rvEPxV",
        "outputId": "3c0ecae7-02db-4b23-9418-ff66dd34e21e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pre-normalization mean: tensor([0.2554, 0.2554, 0.2554])\n",
            "Pre-normalization std : tensor([0.2900, 0.2900, 0.2900])\n",
            "One-batch post-normalization mean: tensor([-0.4267, -0.4267, -0.4267])\n",
            "One-batch post-normalization std : tensor([0.8331, 0.8331, 0.8331])\n",
            "Post-normalization mean (whole dataset): tensor([-5.6691e-08, -5.6691e-08, -5.6691e-08])\n",
            "Post-normalization std  (whole dataset): tensor([1.0000, 1.0000, 1.0000])\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "class UnlabeledImageDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        self.root_dir = Path(root_dir)\n",
        "        self.transform = transform\n",
        "        self.image_paths = [p for p in self.root_dir.iterdir() if p.suffix.lower() in {'.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.tif'}]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = Image.open(self.image_paths[idx]).convert('RGB')\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        return img\n",
        "\n",
        "# 1. Define paths\n",
        "root = \"/content/drive/MyDrive/Research Project 2025/Preprocessed Datasets/Samples/Target - VS Unlabelled/Images\"\n",
        "\n",
        "# 2. Base loader to compute pre-normalization mean & std\n",
        "base_ds = UnlabeledImageDataset(root, transform=transforms.ToTensor())\n",
        "base_loader = DataLoader(base_ds, batch_size=64, shuffle=False, num_workers=2)\n",
        "\n",
        "sum_ = torch.zeros(3)\n",
        "sum_sq = torch.zeros(3)\n",
        "total_pixels = 0\n",
        "\n",
        "for imgs in base_loader:\n",
        "    batch_pixels = imgs.size(0) * imgs.size(2) * imgs.size(3)\n",
        "    total_pixels += batch_pixels\n",
        "    sum_ += imgs.sum(dim=[0, 2, 3])\n",
        "    sum_sq += (imgs ** 2).sum(dim=[0, 2, 3])\n",
        "\n",
        "mean_pre = sum_ / total_pixels\n",
        "std_pre = torch.sqrt(sum_sq / total_pixels - mean_pre**2)\n",
        "\n",
        "print(\"Pre-normalization mean:\", mean_pre)\n",
        "print(\"Pre-normalization std :\", std_pre)\n",
        "\n",
        "# 3. Transformation including normalization\n",
        "norm_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean_pre.tolist(), std_pre.tolist())\n",
        "])\n",
        "\n",
        "# 4. Loader for post-normalization checks\n",
        "norm_ds = UnlabeledImageDataset(root, transform=norm_transform)\n",
        "norm_loader = DataLoader(norm_ds, batch_size=64, shuffle=False, num_workers=2)\n",
        "\n",
        "# 5. One-batch check\n",
        "imgs = next(iter(norm_loader))\n",
        "print(\"One-batch post-normalization mean:\", imgs.mean(dim=[0, 2, 3]))\n",
        "print(\"One-batch post-normalization std :\", imgs.std(dim=[0, 2, 3]))\n",
        "\n",
        "# 6. Full dataset post-normalization stats\n",
        "sum_norm = torch.zeros(3)\n",
        "sum_norm_sq = torch.zeros(3)\n",
        "total_pixels = 0\n",
        "\n",
        "for imgs in norm_loader:\n",
        "    batch_pixels = imgs.size(0) * imgs.size(2) * imgs.size(3)\n",
        "    total_pixels += batch_pixels\n",
        "    sum_norm += imgs.sum(dim=[0, 2, 3])\n",
        "    sum_norm_sq += (imgs ** 2).sum(dim=[0, 2, 3])\n",
        "\n",
        "mean_post = sum_norm / total_pixels\n",
        "std_post = torch.sqrt(sum_norm_sq / total_pixels - mean_post**2)\n",
        "\n",
        "print(\"Post-normalization mean (whole dataset):\", mean_post)\n",
        "print(\"Post-normalization std  (whole dataset):\", std_post)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8niN7gQI3vaJ"
      },
      "source": [
        "10) Let us do Domain Adaptation with DANN using VGG-19's feature extractor for 10 epochs in training.\n",
        "\n",
        "Inspired by: https://jmlr.org/papers/v17/15-239.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Nh7G7o23vot",
        "outputId": "470b829d-921d-4777-ce3c-b682fd1cc10e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Checkpoints x lambdas (DANN): 100%|██████████| 5/5 [1:32:38<00:00, 1111.79s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Saved metrics to: /content/drive/MyDrive/Research Project 2025/Results/Samples/VGG-19/Domain Adaptation/VGG_19-DANN/Performance Metrics/VGG19_DANN_metrics.xlsx\n",
            "✅ Confusion matrices in: /content/drive/MyDrive/Research Project 2025/Results/Samples/VGG-19/Domain Adaptation/VGG_19-DANN/Confusion Matrices\n"
          ]
        }
      ],
      "source": [
        "import os, random, numpy as np, pandas as pd\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch, torch.nn as nn, torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import transforms, datasets\n",
        "\n",
        "import matplotlib.pyplot as plt, seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support, roc_auc_score\n",
        "\n",
        "# ─── Reproducibility ──────────────────────────────────────\n",
        "SEED = 1906525\n",
        "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
        "random.seed(SEED); np.random.seed(SEED)\n",
        "torch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# ─── Paths ───────────────────────────────\n",
        "SRC_DIR = \"/content/drive/MyDrive/Research Project 2025/Preprocessed Datasets/Samples/Source - MedMNIST Labelled/Images\"\n",
        "CKPT_DIR = \"/content/drive/MyDrive/Research Project 2025/Results/Samples/VGG-19/Domain Adaptation/Top-5 Souce Checkpoints\"\n",
        "TGT_IMG_ROOT = \"/content/drive/MyDrive/Research Project 2025/Preprocessed Datasets/Samples/Target - VS Unlabelled/Images\"\n",
        "TGT_CSV = \"/content/drive/MyDrive/Research Project 2025/Preprocessed Datasets/Samples/Target - VS Unlabelled/Target sample labels.csv\"\n",
        "\n",
        "CONF_DIR = \"/content/drive/MyDrive/Research Project 2025/Results/Samples/VGG-19/Domain Adaptation/VGG_19-DANN/Confusion Matrices\"\n",
        "METRICS_DIR = \"/content/drive/MyDrive/Research Project 2025/Results/Samples/VGG-19/Domain Adaptation/VGG_19-DANN/Performance Metrics\"\n",
        "os.makedirs(CONF_DIR, exist_ok=True); os.makedirs(METRICS_DIR, exist_ok=True)\n",
        "\n",
        "# ─── Normalization (per-branch) ──────────────────────────\n",
        "mean_src = [0.3542, 0.3542, 0.3542]; std_src  = [0.2794, 0.2794, 0.2794]\n",
        "mean_tgt = [0.2554, 0.2554, 0.2554]; std_tgt  = [0.2900, 0.2900, 0.2900]\n",
        "\n",
        "tx_src = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean_src, std_src)])\n",
        "tx_tgt = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean_tgt, std_tgt)])\n",
        "\n",
        "# ─── VGG‑19 features builder (cfg E) ─────────────────────\n",
        "cfgs = {\"E\":[64,64,\"M\",128,128,\"M\",256,256,256,256,\"M\",512,512,512,512,\"M\",512,512,512,512,\"M\"]}\n",
        "def make_layers(cfg):\n",
        "    layers, in_c = [], 3\n",
        "    for v in cfg:\n",
        "        if v == \"M\": layers.append(nn.MaxPool2d(2,2))\n",
        "        else:\n",
        "            layers += [nn.Conv2d(in_c, v, 3, padding=1), nn.ReLU(True)]\n",
        "            in_c = v\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "# ─── GRL (λ scales reversed gradient; don't also weight the loss) ─────────\n",
        "from torch.autograd import Function\n",
        "class GradReverse(Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, x, lambd):\n",
        "        ctx.lambd = lambd\n",
        "        return x.view_as(x)\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        return grad_output.neg() * ctx.lambd, None\n",
        "def grad_reverse(x, lambd=1.0): return GradReverse.apply(x, lambd)\n",
        "\n",
        "# ─── DANN head (shared features; task head; domain head) ──────────────────\n",
        "class DANN(nn.Module):\n",
        "    def __init__(self, features, num_classes, bottleneck_dim=256):\n",
        "        super().__init__()\n",
        "        self.features = features\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((7,7))\n",
        "        self.bottleneck = nn.Sequential(\n",
        "            nn.Linear(512*7*7, bottleneck_dim),\n",
        "            nn.BatchNorm1d(bottleneck_dim),\n",
        "            nn.ReLU(True),\n",
        "        )\n",
        "        self.cls_head = nn.Linear(bottleneck_dim, num_classes)\n",
        "        self.dom_head = nn.Sequential(nn.Linear(bottleneck_dim,100), nn.ReLU(True), nn.Linear(100,2))\n",
        "    def forward(self, x, lambda_=0.0):\n",
        "        f = self.features(x)\n",
        "        f = self.avgpool(f); f = torch.flatten(f, 1)\n",
        "        z = self.bottleneck(f)\n",
        "        y_logits = self.cls_head(z)                      # task logits\n",
        "        z_rev = grad_reverse(z, lambda_)\n",
        "        d_logits = self.dom_head(z_rev)                  # domain logits\n",
        "        return y_logits, d_logits\n",
        "\n",
        "# ─── Load VGG‑19 features from our full classifier checkpoints ───────────\n",
        "def load_vgg19_features(ckpt_path, num_classes):\n",
        "    features = make_layers(cfgs[\"E\"])\n",
        "    model = nn.Sequential(\n",
        "        features, nn.AdaptiveAvgPool2d((7,7)), nn.Flatten(),\n",
        "        nn.Linear(512*7*7, 4096), nn.ReLU(True), nn.Dropout(),\n",
        "        nn.Linear(4096, 4096), nn.ReLU(True), nn.Dropout(),\n",
        "        nn.Linear(4096, num_classes)\n",
        "    )\n",
        "    sd = torch.load(ckpt_path, map_location=device)\n",
        "    # allow missing final classifier if shapes mismatch\n",
        "    try: model.load_state_dict(sd, strict=True)\n",
        "    except Exception:\n",
        "        # drop last layer keys if needed\n",
        "        for k in list(sd.keys()):\n",
        "            if k.endswith(\"9.weight\") or k.endswith(\"9.bias\"): del sd[k]\n",
        "        model.load_state_dict(sd, strict=False)\n",
        "    return model[0]  # features\n",
        "\n",
        "# ─── Datasets & loaders ───────────────────────────────────────────────────\n",
        "# Source labeled\n",
        "src_ds = datasets.ImageFolder(SRC_DIR, transform=tx_src)\n",
        "CLASS_NAMES = src_ds.classes\n",
        "CLASS_TO_IDX = {c:i for i,c in enumerate(CLASS_NAMES)}\n",
        "N_CLASSES = len(CLASS_NAMES)\n",
        "\n",
        "def src_loader(bs=32, shuffle=True, num_workers=2):\n",
        "    return DataLoader(src_ds, batch_size=bs, shuffle=shuffle, num_workers=num_workers, pin_memory=True)\n",
        "\n",
        "# Target unlabeled from flat folder + CSV for eval\n",
        "class TargetFlatCSV(Dataset):\n",
        "    def __init__(self, root, csv_path, transform):\n",
        "        self.root = root; self.transform = transform\n",
        "        df = pd.read_csv(csv_path)\n",
        "        self.names = df.iloc[:,0].astype(str).tolist()\n",
        "        self.cls_names = df.iloc[:,1].astype(str).tolist()\n",
        "        # map to indices using source class vocabulary\n",
        "        self.cls_idx = [CLASS_TO_IDX[c] for c in self.cls_names]\n",
        "    def __len__(self): return len(self.names)\n",
        "    def __getitem__(self, i):\n",
        "        fp = os.path.join(self.root, self.names[i])\n",
        "        img = Image.open(fp).convert(\"RGB\")\n",
        "        return self.transform(img), self.cls_idx[i], self.names[i]\n",
        "\n",
        "tgt_ds = TargetFlatCSV(TGT_IMG_ROOT, TGT_CSV, tx_tgt)\n",
        "\n",
        "def tgt_loader(bs=32, shuffle=True, num_workers=2):\n",
        "    return DataLoader(tgt_ds, batch_size=bs, shuffle=shuffle, num_workers=num_workers, pin_memory=True)\n",
        "\n",
        "# ─── Metrics helpers ───────────────────────────────────────────────────────\n",
        "def save_confusion_matrix(y_true, y_pred, class_names, save_path):\n",
        "    cm = confusion_matrix(y_true, y_pred, labels=list(range(len(class_names))))\n",
        "    plt.figure(figsize=(10,8))\n",
        "    ax = sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                     xticklabels=class_names, yticklabels=class_names,\n",
        "                     cbar=False, linewidths=.5)\n",
        "    ax.set_xlabel('Predicted'); ax.set_ylabel('Actual'); ax.set_title('Confusion Matrix (Target)')\n",
        "    plt.tight_layout(); plt.savefig(save_path, dpi=220); plt.close()\n",
        "    return cm\n",
        "\n",
        "def specificity_from_cm(cm):\n",
        "    # per-class specificity: TN / (TN + FP)\n",
        "    spec = []\n",
        "    for k in range(cm.shape[0]):\n",
        "        TP = cm[k,k]\n",
        "        FP = cm[:,k].sum() - TP\n",
        "        FN = cm[k,:].sum() - TP\n",
        "        TN = cm.sum() - (TP+FP+FN)\n",
        "        spec.append( TN / (TN + FP + 1e-12) )\n",
        "    return np.array(spec)\n",
        "\n",
        "def macro_auc(y_true, y_proba, n_classes):\n",
        "    # y_true: ints; y_proba: (N, C) softmax probs\n",
        "    y_true_oh = np.eye(n_classes)[np.asarray(y_true)]\n",
        "    return roc_auc_score(y_true_oh, y_proba, average=\"macro\", multi_class=\"ovr\")  # sklearn API\n",
        "    # ref: sklearn roc_auc_score supports multiclass ovr/ovo. See docs.  # noqa\n",
        "\n",
        "# ─── Training + evaluation for one (ckpt, lambda) ──────────────────────────\n",
        "def train_and_eval_dann(ckpt_path, ckpt_name, lambda_val, epochs=10, bs=32, lr=1e-4):\n",
        "    # loaders\n",
        "    Ls = src_loader(bs=bs, shuffle=True)\n",
        "    Lt = tgt_loader(bs=bs, shuffle=True)\n",
        "\n",
        "    # model/opt\n",
        "    feats = load_vgg19_features(ckpt_path, N_CLASSES)\n",
        "    model = DANN(feats, N_CLASSES).to(device)\n",
        "    opt = optim.Adam(model.parameters(), lr=lr)\n",
        "    ce = nn.CrossEntropyLoss()\n",
        "\n",
        "    model.train()\n",
        "    it_tgt = iter(Lt)\n",
        "    for ep in range(epochs):\n",
        "        for x_s, y_s in Ls:\n",
        "            x_s, y_s = x_s.to(device), y_s.to(device)\n",
        "            try:\n",
        "                x_t, _, _ = next(it_tgt)\n",
        "            except StopIteration:\n",
        "                it_tgt = iter(Lt)\n",
        "                x_t, _, _ = next(it_tgt)\n",
        "            x_t = x_t.to(device)\n",
        "\n",
        "            # Classifier on source only\n",
        "            y_logits_s, _ = model(x_s, lambda_=0.0)\n",
        "            L_cls = ce(y_logits_s, y_s)\n",
        "\n",
        "            # Domain on concatenated (source=0, target=1) with GRL scaling = lambda_val\n",
        "            x_dom = torch.cat([x_s, x_t], dim=0)\n",
        "            _, d_logits = model(x_dom, lambda_=lambda_val)  # scale via GRL only\n",
        "            d_labels = torch.cat([\n",
        "                torch.zeros(x_s.size(0), dtype=torch.long),\n",
        "                torch.ones(x_t.size(0), dtype=torch.long)\n",
        "            ], dim=0).to(device)\n",
        "            L_dom = ce(d_logits, d_labels)\n",
        "\n",
        "            loss = L_cls + L_dom  # GRL already applies λ to the gradient\n",
        "            opt.zero_grad(); loss.backward(); opt.step()\n",
        "\n",
        "    # ── Evaluate on ALL target images (order from dataset)\n",
        "    model.eval()\n",
        "    all_true, all_pred, all_prob = [], [], []\n",
        "    with torch.no_grad():\n",
        "        for xb, yb, _names in DataLoader(tgt_ds, batch_size=64, shuffle=False, num_workers=2):\n",
        "            xb = xb.to(device)\n",
        "            y_logits, _ = model(xb, lambda_=0.0)\n",
        "            probs = torch.softmax(y_logits, dim=1)\n",
        "            pred = probs.argmax(dim=1)\n",
        "            all_true.extend(yb.tolist())\n",
        "            all_pred.extend(pred.cpu().tolist())\n",
        "            all_prob.append(probs.cpu().numpy())\n",
        "    all_prob = np.concatenate(all_prob, axis=0)\n",
        "\n",
        "    # ── Metrics\n",
        "    correct = int(np.sum(np.array(all_true)==np.array(all_pred)))\n",
        "    total = len(all_true)\n",
        "    acc = correct / total\n",
        "    miss = 1.0 - acc\n",
        "\n",
        "    # macro precision/recall(FN=Sensitivity)/F1\n",
        "    prec, rec, f1, _ = precision_recall_fscore_support(all_true, all_pred, labels=list(range(N_CLASSES)), average='macro', zero_division=0)\n",
        "    # macro specificity from confusion matrix\n",
        "    cm = save_confusion_matrix(all_true, all_pred, CLASS_NAMES,\n",
        "                               os.path.join(CONF_DIR, f\"{ckpt_name}_{lambda_val}.png\"))\n",
        "    spec_macro = specificity_from_cm(cm).mean()\n",
        "\n",
        "    # macro AUC (OvR)\n",
        "    try:\n",
        "        auc_macro = macro_auc(all_true, all_prob, N_CLASSES)\n",
        "    except Exception:\n",
        "        auc_macro = float('nan')  # if some classes never appear / degenerate\n",
        "\n",
        "    row = {\n",
        "        \"Checkpoint\": ckpt_name,\n",
        "        \"GAN Type\": \"DANN\",\n",
        "        \"DA Hyperparameter\": \"lambda\",\n",
        "        \"DA Hyperparameter Value\": lambda_val,\n",
        "        \"Correctly Identified Images\": correct,\n",
        "        \"Incorrectly Identified Images\": total - correct,\n",
        "        \"Image Classification Accuracy\": acc,\n",
        "        \"Image Miss Rate\": miss,\n",
        "        \"Precision (macro)\": prec,\n",
        "        \"Sensitivity/Recall (macro)\": rec,\n",
        "        \"Specificity (macro)\": spec_macro,\n",
        "        \"F1-Score (macro)\": f1,\n",
        "        \"AUC-ROC (macro OvR)\": auc_macro\n",
        "    }\n",
        "    return row\n",
        "\n",
        "# ─── Run all checkpoints × lambdas ─────────────────────────────────────────\n",
        "lambda_vals = [0.01, 0.05, 0.1, 0.5, 1.0]\n",
        "ckpt_files = [f for f in os.listdir(CKPT_DIR) if f.endswith(\".pth\")]\n",
        "results = []\n",
        "\n",
        "for fname in tqdm(ckpt_files, desc=\"Checkpoints x lambdas (DANN)\"):\n",
        "    ckpt_path = os.path.join(CKPT_DIR, fname)\n",
        "    ckpt_name = os.path.splitext(fname)[0].replace(\" \", \"\").replace(\"-\", \"\").replace(\"__\", \"_\")\n",
        "    for lam in lambda_vals:\n",
        "        row = train_and_eval_dann(ckpt_path, ckpt_name, lam, epochs=10, bs=32, lr=1e-4)\n",
        "        results.append(row)\n",
        "\n",
        "# ─── Save Excel with 'Checkpoint' before 'DA Hyperparameter' ──────────────\n",
        "df = pd.DataFrame(results)\n",
        "\n",
        "# enforce column order\n",
        "cols = list(df.columns)\n",
        "# Move 'Checkpoint' to before 'DA Hyperparameter'\n",
        "for col in [\"Checkpoint\",\"GAN Type\",\"DA Hyperparameter\",\"DA Hyperparameter Value\"]:\n",
        "    assert col in cols, f\"Missing column {col}\"\n",
        "\n",
        "ordered = (\n",
        "    [\"Checkpoint\",\"GAN Type\",\"DA Hyperparameter\",\"DA Hyperparameter Value\"] +\n",
        "    [c for c in df.columns if c not in [\"Checkpoint\",\"GAN Type\",\"DA Hyperparameter\",\"DA Hyperparameter Value\"]]\n",
        ")\n",
        "df = df[ordered]\n",
        "\n",
        "save_xlsx = os.path.join(METRICS_DIR, \"VGG19_DANN_metrics.xlsx\")\n",
        "df.to_excel(save_xlsx, index=False)\n",
        "print(f\"✅ Saved metrics to: {save_xlsx}\")\n",
        "print(f\"✅ Confusion matrices in: {CONF_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "11) Let us do Domain Adaptation with ADDA using VGG-19's feature extractor.\n",
        "\n",
        "ADDA is inspired by https://openaccess.thecvf.com/content_cvpr_2017/papers/Tzeng_Adversarial_Discriminative_Domain_CVPR_2017_paper.pdf."
      ],
      "metadata": {
        "id": "T_qBqgYXdarp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, re, random, numpy as np, pandas as pd\n",
        "from copy import deepcopy\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch, torch.nn as nn, torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import transforms, datasets\n",
        "\n",
        "import matplotlib.pyplot as plt, seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support, roc_auc_score\n",
        "\n",
        "# ───────────────────────────── Reproducibility ─────────────────────────────\n",
        "SEED = 1906525\n",
        "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
        "random.seed(SEED); np.random.seed(SEED)\n",
        "torch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# ──────────────────────────────── Paths ────────────────────────────────────\n",
        "SRC_DIR  = \"/content/drive/MyDrive/Research Project 2025/Preprocessed Datasets/Samples/Source - MedMNIST Labelled/Images\"\n",
        "CKPT_DIR = \"/content/drive/MyDrive/Research Project 2025/Results/Samples/VGG-19/Domain Adaptation/Top-5 Souce Checkpoints\"\n",
        "\n",
        "TGT_IMG_ROOT = \"/content/drive/MyDrive/Research Project 2025/Preprocessed Datasets/Samples/Target - VS Unlabelled/Images\"\n",
        "TGT_CSV      = \"/content/drive/MyDrive/Research Project 2025/Preprocessed Datasets/Samples/Target - VS Unlabelled/Target sample labels.csv\"\n",
        "\n",
        "CONF_DIR    = \"/content/drive/MyDrive/Research Project 2025/Results/Samples/VGG-19/Domain Adaptation/VGG_19-ADDA/Confusion Matrices\"\n",
        "METRICS_DIR = \"/content/drive/MyDrive/Research Project 2025/Results/Samples/VGG-19/Domain Adaptation/VGG_19-ADDA/Performance Metrics\"\n",
        "os.makedirs(CONF_DIR, exist_ok=True); os.makedirs(METRICS_DIR, exist_ok=True)\n",
        "\n",
        "# ─────────────────────────── Normalization (per branch) ────────────────────\n",
        "mean_src = [0.3542, 0.3542, 0.3542]; std_src  = [0.2794, 0.2794, 0.2794]\n",
        "mean_tgt = [0.2554, 0.2554, 0.2554]; std_tgt  = [0.2900, 0.2900, 0.2900]\n",
        "\n",
        "tx_src = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean_src, std_src)])\n",
        "tx_tgt = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean_tgt, std_tgt)])\n",
        "\n",
        "# ─────────────────────────── VGG-19 blocks we need ─────────────────────────\n",
        "# Encoder = conv features + avgpool + Flatten + FC4096 + FC4096   (outputs 4096-d)\n",
        "# ClassifierHead = Linear(4096 -> num_classes)\n",
        "cfgs = {\"E\":[64,64,\"M\",128,128,\"M\",256,256,256,256,\"M\",512,512,512,512,\"M\",512,512,512,512,\"M\"]}\n",
        "\n",
        "def make_layers(cfg):\n",
        "    layers, in_c = [], 3\n",
        "    for v in cfg:\n",
        "        if v == \"M\":\n",
        "            layers.append(nn.MaxPool2d(2,2))\n",
        "        else:\n",
        "            layers += [nn.Conv2d(in_c, v, 3, padding=1), nn.ReLU(True)]\n",
        "            in_c = v\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "class VGG19Encoder(nn.Module):\n",
        "    def __init__(self, features):\n",
        "        super().__init__()\n",
        "        self.features = features\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((7,7))\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(512*7*7, 4096), nn.ReLU(True), nn.Dropout(),\n",
        "            nn.Linear(4096, 4096), nn.ReLU(True), nn.Dropout()\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = self.fc(x)          # (N, 4096)\n",
        "        return x\n",
        "\n",
        "class ClassifierHead(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Linear(4096, num_classes)\n",
        "    def forward(self, z):\n",
        "        return self.fc(z)\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    # Domain: source=1, target=0  (BCEWithLogits)\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(4096, 1024), nn.ReLU(True), nn.Dropout(0.5),\n",
        "            nn.Linear(1024, 1024), nn.ReLU(True), nn.Dropout(0.5),\n",
        "            nn.Linear(1024, 1)\n",
        "        )\n",
        "    def forward(self, z):\n",
        "        return self.net(z).view(-1)\n",
        "\n",
        "# Convenience: full source model structure to load checkpoints strictly\n",
        "class _FullVGG19ForLoad(nn.Module):\n",
        "    # features → avgpool → Flatten → 4096 → 4096 → num_classes\n",
        "    def __init__(self, num_classes):\n",
        "        super().__init__()\n",
        "        self.features = make_layers(cfgs[\"E\"])\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((7,7))\n",
        "        self.seq = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(512*7*7, 4096), nn.ReLU(True), nn.Dropout(),\n",
        "            nn.Linear(4096, 4096), nn.ReLU(True), nn.Dropout(),\n",
        "            nn.Linear(4096, num_classes)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.avgpool(x)\n",
        "        return self.seq(x)\n",
        "\n",
        "def load_src_encoder_and_classifier(ckpt_path, num_classes):\n",
        "    full = _FullVGG19ForLoad(num_classes)\n",
        "    sd = torch.load(ckpt_path, map_location='cpu')\n",
        "    # strict load; if last layer size mismatched in old ckpt, relax just that\n",
        "    try:\n",
        "        full.load_state_dict(sd, strict=True)\n",
        "    except Exception:\n",
        "        # drop final layer if mismatch, then load non-strict\n",
        "        for k in list(sd.keys()):\n",
        "            if k.endswith(\".6.weight\") or k.endswith(\".6.bias\"):\n",
        "                del sd[k]\n",
        "        full.load_state_dict(sd, strict=False)\n",
        "\n",
        "    # split into Encoder (till penultimate layer) + Classifier head\n",
        "    encoder = VGG19Encoder(full.features)\n",
        "    # copy fc weights from the loaded model\n",
        "    encoder.avgpool.load_state_dict(full.avgpool.state_dict())\n",
        "    # copy Flatten + FC4096 + FC4096 from 'seq' excluding final (index 6)\n",
        "    # full.seq: [Flatten, Lin, ReLU, Drop, Lin, ReLU, Drop, Lin(num_classes)]\n",
        "    enc_seq = nn.Sequential(*list(full.seq.children())[:-1])  # drop final Linear\n",
        "    encoder.fc.load_state_dict(enc_seq.state_dict())\n",
        "\n",
        "    classifier = ClassifierHead(num_classes)\n",
        "    # grab last Linear from full.seq\n",
        "    classifier.fc.load_state_dict(list(full.seq.children())[-1].state_dict())\n",
        "    return encoder.to(device), classifier.to(device)\n",
        "\n",
        "# ───────────────────────────── Datasets & Loaders ──────────────────────────\n",
        "src_ds = datasets.ImageFolder(SRC_DIR, transform=tx_src)\n",
        "CLASS_NAMES = src_ds.classes\n",
        "CLASS_TO_IDX = {c:i for i,c in enumerate(CLASS_NAMES)}\n",
        "N_CLASSES = len(CLASS_NAMES)\n",
        "\n",
        "def src_loader(bs=32, shuffle=True, num_workers=2):\n",
        "    return DataLoader(src_ds, batch_size=bs, shuffle=shuffle, num_workers=num_workers, pin_memory=True)\n",
        "\n",
        "class TargetFlatCSV(Dataset):\n",
        "    # CSV: [image_name, class_name]\n",
        "    def __init__(self, root, csv_path, transform):\n",
        "        self.root = root; self.transform = transform\n",
        "        df = pd.read_csv(csv_path)\n",
        "        self.names = df.iloc[:,0].astype(str).tolist()\n",
        "        self.cls_names = df.iloc[:,1].astype(str).tolist()\n",
        "        self.cls_idx = [CLASS_TO_IDX[c] for c in self.cls_names]\n",
        "    def __len__(self): return len(self.names)\n",
        "    def __getitem__(self, i):\n",
        "        fp = os.path.join(self.root, self.names[i])\n",
        "        img = Image.open(fp).convert(\"RGB\")\n",
        "        return self.transform(img), self.cls_idx[i], self.names[i]\n",
        "\n",
        "tgt_ds = TargetFlatCSV(TGT_IMG_ROOT, TGT_CSV, tx_tgt)\n",
        "\n",
        "def tgt_loader(bs=32, shuffle=True, num_workers=2):\n",
        "    return DataLoader(tgt_ds, batch_size=bs, shuffle=shuffle, num_workers=num_workers, pin_memory=True)\n",
        "\n",
        "# ──────────────────────────────── Metrics utils ────────────────────────────\n",
        "def save_confusion_matrix(y_true, y_pred, class_names, save_path):\n",
        "    cm = confusion_matrix(y_true, y_pred, labels=list(range(len(class_names))))\n",
        "    plt.figure(figsize=(10,8))\n",
        "    ax = sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                     xticklabels=class_names, yticklabels=class_names,\n",
        "                     cbar=False, linewidths=.5)\n",
        "    ax.set_xlabel('Predicted'); ax.set_ylabel('Actual'); ax.set_title('Confusion Matrix (Target)')\n",
        "    plt.tight_layout(); plt.savefig(save_path, dpi=220); plt.close()\n",
        "    return cm\n",
        "\n",
        "def specificity_from_cm(cm):\n",
        "    spec = []\n",
        "    for k in range(cm.shape[0]):\n",
        "        TP = cm[k,k]\n",
        "        FP = cm[:,k].sum() - TP\n",
        "        FN = cm[k,:].sum() - TP\n",
        "        TN = cm.sum() - (TP+FP+FN)\n",
        "        spec.append( TN / (TN + FP + 1e-12) )\n",
        "    return np.array(spec)\n",
        "\n",
        "def macro_auc(y_true, y_proba, n_classes):\n",
        "    # y_proba: (N, C) softmax probs\n",
        "    y_true_oh = np.eye(n_classes)[np.asarray(y_true)]\n",
        "    return roc_auc_score(y_true_oh, y_proba, average=\"macro\", multi_class=\"ovr\")\n",
        "\n",
        "# ─────────────────────────────── ADDA Training ─────────────────────────────\n",
        "def infinite(dl):\n",
        "    while True:\n",
        "        for b in dl: yield b\n",
        "\n",
        "def lr_name(lr):\n",
        "    # exact\n",
        "    mapping = {1e-3:\"1e-3\", 5e-4:\"5e-4\", 2e-4:\"2e-4\", 1e-4:\"1e-4\", 5e-5:\"5e-5\"}\n",
        "    return mapping.get(lr, f\"{lr:.0e}\".replace(\"e-0\",\"e-\"))\n",
        "\n",
        "def sanitize(s):\n",
        "    # keep letters, digits and underscores for ckpt names\n",
        "    s = s.replace(\" \", \"_\")\n",
        "    s = re.sub(r\"[^A-Za-z0-9_]+\", \"\", s)\n",
        "    return s\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_on_target(tgt_encoder, src_classifier, bs=64):\n",
        "    tgt_encoder.eval(); src_classifier.eval()\n",
        "    y_true, y_pred, probs = [], [], []\n",
        "\n",
        "    for xb, yb, _ in DataLoader(tgt_ds, batch_size=bs, shuffle=False, num_workers=2):\n",
        "        xb = xb.to(device)\n",
        "        feats = tgt_encoder(xb)\n",
        "        logits = src_classifier(feats)\n",
        "        p = torch.softmax(logits, dim=1)\n",
        "        pred = p.argmax(dim=1)\n",
        "\n",
        "        y_true.extend(yb.tolist())\n",
        "        y_pred.extend(pred.cpu().tolist())\n",
        "        probs.append(p.cpu().numpy())\n",
        "\n",
        "    probs = np.concatenate(probs, axis=0) if len(probs)>0 else np.zeros((0, N_CLASSES))\n",
        "    return y_true, y_pred, probs\n",
        "\n",
        "def train_adda_for_ckpt_lr(ckpt_path, ckpt_name, lr, epochs=10, bs=32): #change epochs\n",
        "    # Phase A (already done offline): Source pretraining\n",
        "    # Load source encoder & classifier from checkpoint; freeze them.\n",
        "    src_encoder, src_classifier = load_src_encoder_and_classifier(ckpt_path, N_CLASSES)\n",
        "    for p in list(src_encoder.parameters()) + list(src_classifier.parameters()):\n",
        "        p.requires_grad_(False)\n",
        "    src_encoder.eval(); src_classifier.eval()\n",
        "\n",
        "    # Phase B (ADDA): initialize target encoder from source encoder; train adversarially\n",
        "    tgt_encoder = deepcopy(src_encoder).train().to(device)\n",
        "    D = Discriminator().to(device)\n",
        "\n",
        "    opt_G = optim.Adam(tgt_encoder.parameters(), lr=lr, betas=(0.5, 0.999))\n",
        "    opt_D = optim.Adam(D.parameters(),           lr=lr, betas=(0.5, 0.999))\n",
        "    bce   = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    Ls = src_loader(bs=bs, shuffle=True)\n",
        "    Lt = tgt_loader(bs=bs, shuffle=True)\n",
        "    it_s, it_t = infinite(Ls), infinite(Lt)\n",
        "\n",
        "    steps_per_epoch = min(len(Ls), len(Lt))\n",
        "\n",
        "    for ep in range(epochs):\n",
        "        tgt_encoder.train(); D.train()\n",
        "        loop = tqdm(range(steps_per_epoch), desc=f\"[ADDA] {ckpt_name} | lr={lr_name(lr)} | ep {ep+1}/{epochs}\", leave=False)\n",
        "        for _ in loop:\n",
        "            # ── sample source & target\n",
        "            xs, ys = next(it_s); xt, _, _ = next(it_t)\n",
        "            xs = xs.to(device); xt = xt.to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                z_s = src_encoder(xs)    # fixed\n",
        "            z_t = tgt_encoder(xt)        # trainable\n",
        "\n",
        "            # ── (1) Train Discriminator: D(z_s)=1, D(z_t)=0\n",
        "            opt_D.zero_grad(set_to_none=True)\n",
        "            d_src = D(z_s.detach()); lbl_src = torch.ones_like(d_src)\n",
        "            d_tgt = D(z_t.detach()); lbl_tgt = torch.zeros_like(d_tgt)\n",
        "            loss_D = (bce(d_src, lbl_src) + bce(d_tgt, lbl_tgt)) * 0.5\n",
        "            loss_D.backward(); opt_D.step()\n",
        "\n",
        "            # ── (2) Train Target Encoder (Generator side): fool D → want D(z_t)=1\n",
        "            opt_G.zero_grad(set_to_none=True)\n",
        "            d_tgt_for_g = D(z_t)               # reuse latest z_t\n",
        "            lbl_trick   = torch.ones_like(d_tgt_for_g)\n",
        "            loss_G = bce(d_tgt_for_g, lbl_trick)\n",
        "            loss_G.backward(); opt_G.step()\n",
        "            loop.set_postfix({\"loss_D\": float(loss_D), \"loss_G\": float(loss_G)})\n",
        "\n",
        "    # ── Evaluate target using frozen source classifier\n",
        "    y_true, y_pred, y_prob = eval_on_target(tgt_encoder, src_classifier, bs=64)\n",
        "\n",
        "    total = len(y_true)\n",
        "    correct = int(np.sum(np.array(y_true)==np.array(y_pred)))\n",
        "    acc = correct / (total if total>0 else 1.0)\n",
        "    miss = 1.0 - acc\n",
        "\n",
        "    prec, rec, f1, _ = precision_recall_fscore_support(\n",
        "        y_true, y_pred, labels=list(range(N_CLASSES)), average='macro', zero_division=0\n",
        "    )\n",
        "\n",
        "    cm = save_confusion_matrix(\n",
        "        y_true, y_pred, CLASS_NAMES,\n",
        "        os.path.join(CONF_DIR, f\"{sanitize(ckpt_name)}_{lr_name(lr)}.png\")\n",
        "    )\n",
        "    spec_macro = specificity_from_cm(cm).mean()\n",
        "\n",
        "    try:\n",
        "        auc_macro = macro_auc(y_true, y_prob, N_CLASSES)\n",
        "    except Exception:\n",
        "        auc_macro = float('nan')  # handle degenerate cases safely\n",
        "\n",
        "    row = {\n",
        "        \"Checkpoint\": ckpt_name,\n",
        "        \"GAN Type\": \"ADDA\",\n",
        "        \"DA Hyperparameter\": \"lr\",\n",
        "        \"DA Hyperparameter Value\": lr_name(lr),\n",
        "        \"Correctly Identified Images\": correct,\n",
        "        \"Incorrectly Identified Images\": total - correct,\n",
        "        \"Image Classification Accuracy\": acc,\n",
        "        \"Classification Rate\": acc,\n",
        "        \"Image Miss Rate\": miss,\n",
        "        \"Precision (macro)\": prec,\n",
        "        \"Sensitivity/Recall (macro)\": rec,\n",
        "        \"Specificity (macro)\": spec_macro,\n",
        "        \"F1-Score (macro)\": f1,\n",
        "        \"AUC-ROC (macro OvR)\": auc_macro\n",
        "    }\n",
        "    return row\n",
        "\n",
        "# ───────────────────────────── Run: ckpts × lr grid ────────────────────────\n",
        "lr_vals = [1e-3, 5e-4, 2e-4, 1e-4, 5e-5]\n",
        "ckpt_files = [f for f in os.listdir(CKPT_DIR) if f.endswith(\".pth\")]\n",
        "results = []\n",
        "\n",
        "for fname in ckpt_files:\n",
        "    ckpt_path = os.path.join(CKPT_DIR, fname)\n",
        "    ckpt_name = os.path.splitext(fname)[0]\n",
        "    for lr in lr_vals:\n",
        "        row = train_adda_for_ckpt_lr(ckpt_path, ckpt_name, lr, epochs=10, bs=32)\n",
        "        results.append(row)\n",
        "\n",
        "# ───────────────────────────── Save Excel (append) ─────────────────────────\n",
        "df_new = pd.DataFrame(results)\n",
        "\n",
        "xlsx_path = os.path.join(METRICS_DIR, \"VGG19_ADDA_metrics.xlsx\")\n",
        "if os.path.exists(xlsx_path):\n",
        "    try:\n",
        "        df_old = pd.read_excel(xlsx_path)\n",
        "        df_out = pd.concat([df_old, df_new], ignore_index=True)\n",
        "    except Exception:\n",
        "        df_out = df_new.copy()\n",
        "else:\n",
        "    df_out = df_new.copy()\n",
        "\n",
        "# Ensure the column ordering: 'Checkpoint' before 'DA Hyperparameter'\n",
        "ordered = (\n",
        "    [\"Checkpoint\",\"GAN Type\",\"DA Hyperparameter\",\"DA Hyperparameter Value\"] +\n",
        "    [c for c in df_out.columns if c not in [\"Checkpoint\",\"GAN Type\",\"DA Hyperparameter\",\"DA Hyperparameter Value\"]]\n",
        ")\n",
        "df_out = df_out[ordered]\n",
        "df_out.to_excel(xlsx_path, index=False)\n",
        "\n",
        "print(f\"✅ Saved metrics to: {xlsx_path}\")\n",
        "print(f\"✅ Confusion matrices saved in: {CONF_DIR}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E7EhGcgeda9Q",
        "outputId": "30742e17-38e2-45ce-bd7a-ebefb1ad2edb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Saved metrics to: /content/drive/MyDrive/Research Project 2025/Results/Samples/VGG-19/Domain Adaptation/VGG_19-ADDA/Performance Metrics/VGG19_ADDA_metrics.xlsx\n",
            "✅ Confusion matrices saved in: /content/drive/MyDrive/Research Project 2025/Results/Samples/VGG-19/Domain Adaptation/VGG_19-ADDA/Confusion Matrices\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}